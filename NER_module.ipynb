{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "437ea841",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import DistilBertForTokenClassification\n",
    "\n",
    "from torch.optim import AdamW\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import SGD\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn.metrics import accuracy_score,f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2fb9c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "N = 10000\n",
    "#N = 43880\n",
    "df = pd.read_csv(\"NER_dataset.csv\", encoding=\"cp949\").sample(frac=1)[:N]\n",
    "\n",
    "#change columns names\n",
    "df.rename(columns = {'text':'sentence', 'labels':'tags'}, inplace = True)\n",
    "\n",
    "#split train, dev , test sets\n",
    "df_train, df_dev, df_test = np.split(df.sample(frac=1, random_state=42),\n",
    "                            [int(.8 * len(df)), int(.9 * len(df))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cc68dfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Admission Date :</td>\n",
       "      <td>O O O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012-10-31</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Discharge Date :</td>\n",
       "      <td>O O O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-11-07</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Date of Birth :</td>\n",
       "      <td>O O O O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               text    labels\n",
       "0  Admission Date :    O O O \n",
       "1        2012-10-31        O \n",
       "2  Discharge Date :    O O O \n",
       "3        2012-11-07        O \n",
       "4   Date of Birth :  O O O O "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df2 = pd.read_csv('NER_dataset.csv', encoding = \"cp949\")\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fa71c05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43876</th>\n",
       "      <td>05/16/2004</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43877</th>\n",
       "      <td>TD :</td>\n",
       "      <td>O O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43878</th>\n",
       "      <td>05/16/2004 7:28 A 265076</td>\n",
       "      <td>O O O O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43879</th>\n",
       "      <td>cc :</td>\n",
       "      <td>O O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43880</th>\n",
       "      <td>OIE LIMOR WARM , M.D. CH GILD WARM , M.D.</td>\n",
       "      <td>O O O O O O O O O O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            text                labels\n",
       "43876                                 05/16/2004                    O \n",
       "43877                                       TD :                  O O \n",
       "43878                   05/16/2004 7:28 A 265076              O O O O \n",
       "43879                                       cc :                  O O \n",
       "43880  OIE LIMOR WARM , M.D. CH GILD WARM , M.D.  O O O O O O O O O O "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd7e26e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'I-TR', 'I-PR', 'B-TR', 'I-TE', 'B-PR', 'O', 'B-TE'}\n",
      "{'B-PR': 0, 'B-TE': 1, 'B-TR': 2, 'I-PR': 3, 'I-TE': 4, 'I-TR': 5, 'O': 6}\n"
     ]
    }
   ],
   "source": [
    "# Split labels based on whitespace and turn them into a list\n",
    "labels2 = [i.split() for i in df2['labels'].values.tolist()]\n",
    "\n",
    "# Check how many labels are there in the dataset\n",
    "unique_labels2 = set()\n",
    "\n",
    "for lb in labels2:\n",
    "  [unique_labels2.add(i) for i in lb if i not in unique_labels2]\n",
    " \n",
    "print(unique_labels2)\n",
    "# {'B-tim', 'B-art', 'I-art', 'O', 'I-gpe', 'I-per', 'I-nat', 'I-geo', 'B-eve', 'B-org', 'B-gpe', 'I-eve', 'B-per', 'I-tim', 'B-nat', 'B-geo', 'I-org'}\n",
    "\n",
    "# Map each label into its id representation and vice versa\n",
    "labels_to_ids2 = {k: v for v, k in enumerate(sorted(unique_labels2))}\n",
    "ids_to_labels2 = {v: k for v, k in enumerate(sorted(unique_labels2))}\n",
    "print(labels_to_ids2)\n",
    "# {'B-art': 0, 'B-eve': 1, 'B-geo': 2, 'B-gpe': 3, 'B-nat': 4, 'B-org': 5, 'B-per': 6, 'B-tim': 7, 'I-art': 8, 'I-eve': 9, 'I-geo': 10, 'I-gpe': 11, 'I-nat': 12, 'I-org': 13, 'I-per': 14, 'I-tim': 15, 'O': 16}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da4c180a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistilbertNER(nn.Module):\n",
    "  \"\"\"\n",
    "  Implement NN class based on distilbert pretrained from Hugging face.\n",
    "  Inputs : \n",
    "    tokens_dim : int specifyng the dimension of the classifier\n",
    "  \"\"\"\n",
    "  \n",
    "  def __init__(self, tokens_dim):\n",
    "    super(DistilbertNER,self).__init__()\n",
    "    \n",
    "    if type(tokens_dim) != int:\n",
    "            raise TypeError('Please tokens_dim should be an integer')\n",
    "\n",
    "    if tokens_dim <= 0:\n",
    "          raise ValueError('Classification layer dimension should be at least 1')\n",
    "\n",
    "    self.pretrained = DistilBertForTokenClassification.from_pretrained(\"distilbert-base-uncased\", num_labels = tokens_dim) #set the output of each token classifier = unique_lables\n",
    "\n",
    "  def forward(self, input_ids, attention_mask, labels = None): #labels are needed in order to compute the loss\n",
    "    \"\"\"\n",
    "  Forwad computation of the network\n",
    "  Input:\n",
    "    - inputs_ids : from model tokenizer\n",
    "    - attention :  mask from model tokenizer\n",
    "    - labels : if given the model is able to return the loss value\n",
    "  \"\"\"\n",
    "\n",
    "    #inference time no labels\n",
    "    if labels == None:\n",
    "      out = self.pretrained(input_ids = input_ids, attention_mask = attention_mask )\n",
    "      return out\n",
    "\n",
    "    out = self.pretrained(input_ids = input_ids, attention_mask = attention_mask , labels = labels)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3cac910",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NerDataset(torch.utils.data.Dataset):\n",
    "  \"\"\"\n",
    "  Custom dataset implementation to get (text,labels) tuples\n",
    "  Inputs:\n",
    "   - df : dataframe with columns [tags, sentence]\n",
    "  \"\"\"\n",
    "  \n",
    "  def __init__(self, df):\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "      raise TypeError('Input should be a dataframe')\n",
    "    \n",
    "    if \"tags\" not in df.columns or \"sentence\" not in df.columns:\n",
    "      raise ValueError(\"Dataframe should contain 'tags' and 'sentence' columns\")\n",
    "    \n",
    "    tags_list = [i.split() for i in df[\"tags\"].values.tolist()]\n",
    "    texts = df[\"sentence\"].values.tolist()\n",
    "\n",
    "    self.texts = [tokenizer(text, padding = \"max_length\", truncation = True, return_tensors = \"pt\") for text in texts]\n",
    "    self.labels = [match_tokens_labels(text, tags) for text,tags in zip(self.texts, tags_list)]\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.labels)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    batch_text = self.texts[idx]\n",
    "    batch_labels = self.labels[idx]\n",
    "\n",
    "    return batch_text, torch.LongTensor(batch_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47e4bcab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricsTracking():\n",
    "  \"\"\"\n",
    "  In order make the train loop lighter I define this class to track all the metrics that we are going to measure for our model.\n",
    "  \"\"\"\n",
    "  def __init__(self):\n",
    "\n",
    "    self.total_acc = 0\n",
    "    self.total_f1 = 0\n",
    "    self.total_precision = 0\n",
    "    self.total_recall = 0\n",
    "\n",
    "  def update(self, predictions, labels , ignore_token = -100):\n",
    "    '''\n",
    "    Call this function every time you need to update your metrics.\n",
    "    Where in the train there was a -100, were additional token that we dont want to label, so remove them.\n",
    "    If we flatten the batch its easier to access the indexed = -100\n",
    "    '''  \n",
    "    predictions = predictions.flatten()\n",
    "    labels = labels.flatten()\n",
    "    \n",
    "    predictions = predictions[labels != ignore_token]\n",
    "    labels = labels[labels != ignore_token]\n",
    "\n",
    "    predictions = predictions.to(\"cpu\")\n",
    "    labels = labels.to(\"cpu\")\n",
    "\n",
    "    acc = accuracy_score(labels,predictions)\n",
    "    f1 = f1_score(labels, predictions, average = \"macro\")\n",
    "    precision = precision_score(labels, predictions, average = \"macro\")\n",
    "    recall = recall_score(labels, predictions, average = \"macro\")\n",
    "\n",
    "    self.total_acc  += acc\n",
    "    self.total_f1 += f1\n",
    "    self.total_precision += precision\n",
    "    self.total_recall  += recall\n",
    "\n",
    "  def return_avg_metrics(self,data_loader_size):\n",
    "    n = data_loader_size\n",
    "    metrics = {\n",
    "        \"acc\": round(self.total_acc / n ,3), \n",
    "        \"f1\": round(self.total_f1 / n, 3), \n",
    "        \"precision\" : round(self.total_precision / n, 3), \n",
    "        \"recall\": round(self.total_recall / n, 3)\n",
    "          }\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f8f46b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tags_2_labels(tags : str, tag2idx : dict):\n",
    "  '''\n",
    "  Method that takes a list of tags and a dictionary mapping and returns a list of labels (associated).\n",
    "  Used to create the \"label\" column in df from the \"tags\" column.\n",
    "  '''\n",
    "  return [tag2idx[tag] if tag in tag2idx else unseen_label for tag in tags.split()] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16653275",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tags_mapping(tags_series : pd.Series):\n",
    "  \"\"\"\n",
    "  tag_series = df column with tags for each sentence.\n",
    "  Returns:\n",
    "    - dictionary mapping tags to indexes (label)\n",
    "    - dictionary mappign inedexes to tags\n",
    "    - The label corresponding to tag 'O'\n",
    "    - A set of unique tags ecountered in the trainind df, this will define the classifier dimension\n",
    "  \"\"\"\n",
    "\n",
    "  if not isinstance(tags_series, pd.Series):\n",
    "      raise TypeError('Input should be a padas Series')\n",
    "\n",
    "  unique_tags = set()\n",
    "  \n",
    "  for tag_list in df_train[\"tags\"]:\n",
    "    for tag in tag_list.split():\n",
    "      unique_tags.add(tag)\n",
    "\n",
    "  tag2idx = {k:v for v,k in enumerate(sorted(unique_tags))}\n",
    "  idx2tag = {k:v for v,k in tag2idx.items()}\n",
    "\n",
    "  unseen_label = tag2idx[\"O\"]\n",
    "\n",
    "  return tag2idx, idx2tag, unseen_label, unique_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a66e276c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_tokens_labels(tokenized_input, tags, ignore_token = -100):\n",
    "        '''\n",
    "        Used in the custom dataset.\n",
    "        -100 will be tha label used to match additional tokens like [CLS] [PAD] that we dont care about. \n",
    "        Inputs : \n",
    "          - tokenized_input : tokenizer over the imput text -> {input_ids, attention_mask}\n",
    "          - tags : is a single label array -> [O O O O O O O O O O O O O O B-tim O]\n",
    "        \n",
    "        Returns a list of labels that match the tokenized text -> [-100, 3,5,6,-100,...]\n",
    "        '''\n",
    "\n",
    "        #gives an array [ None , 0 , 1 ,2 ,... None]. Each index tells the word of reference of the token\n",
    "        word_ids = tokenized_input.word_ids()\n",
    "\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "\n",
    "        for word_idx in word_ids:\n",
    "\n",
    "            if word_idx is None:\n",
    "                label_ids.append(ignore_token)\n",
    "\n",
    "            #if its equal to the previous word we can add the same label id of the provious or -100 \n",
    "            else :\n",
    "                try:\n",
    "                  reference_tag = tags[word_idx]\n",
    "                  label_ids.append(tag2idx[reference_tag])\n",
    "                except:\n",
    "                  label_ids.append(ignore_token)\n",
    "            \n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        return label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b0c9776",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_model(model,num_layers = 1):\n",
    "  \"\"\"\n",
    "  Freeze last num_layers of a model to prevent ctastrophic forgetting.\n",
    "  Doesn't seem to work weel, its better to fine tune the entire netwok\n",
    "  \"\"\"\n",
    "  for id , params in enumerate(model.parameters()):\n",
    "    if id == len(list(model.parameters())) - num_layers: \n",
    "      print(\"last layer unfreezed\")\n",
    "      params.requires_grad = True\n",
    "    else:\n",
    "      params.requires_grad = False\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6845bbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model, train_dataset, dev_dataset, optimizer,  batch_size, epochs):\n",
    "  \n",
    "  train_dataloader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n",
    "  dev_dataloader = DataLoader(dev_dataset, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "  model = model.to(device)\n",
    "\n",
    "  for epoch in range(epochs) : \n",
    "    \n",
    "    train_metrics = MetricsTracking()\n",
    "    total_loss_train = 0\n",
    "\n",
    "    model.train() #train mode\n",
    "\n",
    "    for train_data, train_label in tqdm(train_dataloader):\n",
    "\n",
    "      train_label = train_label.to(device)\n",
    "      '''\n",
    "      squeeze in order to match the sizes. From [batch,1,seq_len] --> [batch,seq_len] \n",
    "      '''\n",
    "      mask = train_data['attention_mask'].squeeze(1).to(device)\n",
    "      input_id = train_data['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "      \n",
    "      output = model(input_id, mask, train_label)\n",
    "      loss, logits = output.loss, output.logits\n",
    "      predictions = logits.argmax(dim= -1) \n",
    "\n",
    "      #compute metrics\n",
    "      train_metrics.update(predictions, train_label)\n",
    "      total_loss_train += loss.item()\n",
    "\n",
    "      #grad step\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "    \n",
    "    '''\n",
    "    EVALUATION MODE\n",
    "    '''            \n",
    "    model.eval()\n",
    "\n",
    "    dev_metrics = MetricsTracking()\n",
    "    total_loss_dev = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "      for dev_data, dev_label in dev_dataloader:\n",
    "\n",
    "        dev_label = dev_label.to(device)\n",
    "\n",
    "        mask = dev_data['attention_mask'].squeeze(1).to(device)\n",
    "        input_id = dev_data['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "        output = model(input_id, mask, dev_label)\n",
    "        loss, logits = output.loss, output.logits\n",
    "\n",
    "        predictions = logits.argmax(dim= -1)     \n",
    "\n",
    "        dev_metrics.update(predictions, dev_label)\n",
    "        total_loss_dev += loss.item()\n",
    "    \n",
    "    train_results = train_metrics.return_avg_metrics(len(train_dataloader))\n",
    "    dev_results = dev_metrics.return_avg_metrics(len(dev_dataloader))\n",
    "\n",
    "    print(f\"TRAIN \\nLoss: {total_loss_train / len(train_dataset)} \\nMetrics {train_results}\\n\" ) \n",
    "    print(f\"VALIDATION \\nLoss {total_loss_dev / len(dev_dataset)} \\nMetrics{dev_results}\\n\" )   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ed86fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create tag-label mapping\n",
    "tag2idx, idx2tag , unseen_label, unique_tags = tags_mapping(df_train[\"tags\"])\n",
    "\n",
    "#create the label column from tag. Unseen labels will be tagged as \"O\"\n",
    "for df in [df_train, df_dev, df_test]:\n",
    "  df[\"labels\"] = df[\"tags\"].apply(lambda tags : tags_2_labels(tags, tag2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22643578",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#toeknized text\u001b[39;00m\n\u001b[0;32m      5\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistilbert-base-uncased\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m text_tokenized \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_length\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#mapping token to original word\u001b[39;00m\n\u001b[0;32m      9\u001b[0m word_ids \u001b[38;5;241m=\u001b[39m text_tokenized\u001b[38;5;241m.\u001b[39mword_ids()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\tokenization_utils_base.py:2488\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[1;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2486\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[0;32m   2487\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[1;32m-> 2488\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_one(text\u001b[38;5;241m=\u001b[39mtext, text_pair\u001b[38;5;241m=\u001b[39mtext_pair, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mall_kwargs)\n\u001b[0;32m   2489\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2490\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\tokenization_utils_base.py:2574\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2569\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2570\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch length of `text`: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not match batch length of `text_pair`:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2571\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text_pair)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2572\u001b[0m         )\n\u001b[0;32m   2573\u001b[0m     batch_text_or_text_pairs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(text, text_pair)) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m text\n\u001b[1;32m-> 2574\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_encode_plus(\n\u001b[0;32m   2575\u001b[0m         batch_text_or_text_pairs\u001b[38;5;241m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[0;32m   2576\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m   2577\u001b[0m         padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[0;32m   2578\u001b[0m         truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   2579\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[0;32m   2580\u001b[0m         stride\u001b[38;5;241m=\u001b[39mstride,\n\u001b[0;32m   2581\u001b[0m         is_split_into_words\u001b[38;5;241m=\u001b[39mis_split_into_words,\n\u001b[0;32m   2582\u001b[0m         pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m   2583\u001b[0m         return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors,\n\u001b[0;32m   2584\u001b[0m         return_token_type_ids\u001b[38;5;241m=\u001b[39mreturn_token_type_ids,\n\u001b[0;32m   2585\u001b[0m         return_attention_mask\u001b[38;5;241m=\u001b[39mreturn_attention_mask,\n\u001b[0;32m   2586\u001b[0m         return_overflowing_tokens\u001b[38;5;241m=\u001b[39mreturn_overflowing_tokens,\n\u001b[0;32m   2587\u001b[0m         return_special_tokens_mask\u001b[38;5;241m=\u001b[39mreturn_special_tokens_mask,\n\u001b[0;32m   2588\u001b[0m         return_offsets_mapping\u001b[38;5;241m=\u001b[39mreturn_offsets_mapping,\n\u001b[0;32m   2589\u001b[0m         return_length\u001b[38;5;241m=\u001b[39mreturn_length,\n\u001b[0;32m   2590\u001b[0m         verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m   2591\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2592\u001b[0m     )\n\u001b[0;32m   2593\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2594\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_plus(\n\u001b[0;32m   2595\u001b[0m         text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[0;32m   2596\u001b[0m         text_pair\u001b[38;5;241m=\u001b[39mtext_pair,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2612\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2613\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\tokenization_utils_base.py:2765\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_encode_plus\u001b[1;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2755\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[0;32m   2756\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[0;32m   2757\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[0;32m   2758\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2762\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2763\u001b[0m )\n\u001b[1;32m-> 2765\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_encode_plus(\n\u001b[0;32m   2766\u001b[0m     batch_text_or_text_pairs\u001b[38;5;241m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[0;32m   2767\u001b[0m     add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m   2768\u001b[0m     padding_strategy\u001b[38;5;241m=\u001b[39mpadding_strategy,\n\u001b[0;32m   2769\u001b[0m     truncation_strategy\u001b[38;5;241m=\u001b[39mtruncation_strategy,\n\u001b[0;32m   2770\u001b[0m     max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[0;32m   2771\u001b[0m     stride\u001b[38;5;241m=\u001b[39mstride,\n\u001b[0;32m   2772\u001b[0m     is_split_into_words\u001b[38;5;241m=\u001b[39mis_split_into_words,\n\u001b[0;32m   2773\u001b[0m     pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m   2774\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors,\n\u001b[0;32m   2775\u001b[0m     return_token_type_ids\u001b[38;5;241m=\u001b[39mreturn_token_type_ids,\n\u001b[0;32m   2776\u001b[0m     return_attention_mask\u001b[38;5;241m=\u001b[39mreturn_attention_mask,\n\u001b[0;32m   2777\u001b[0m     return_overflowing_tokens\u001b[38;5;241m=\u001b[39mreturn_overflowing_tokens,\n\u001b[0;32m   2778\u001b[0m     return_special_tokens_mask\u001b[38;5;241m=\u001b[39mreturn_special_tokens_mask,\n\u001b[0;32m   2779\u001b[0m     return_offsets_mapping\u001b[38;5;241m=\u001b[39mreturn_offsets_mapping,\n\u001b[0;32m   2780\u001b[0m     return_length\u001b[38;5;241m=\u001b[39mreturn_length,\n\u001b[0;32m   2781\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m   2782\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2783\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\tokenization_utils_fast.py:429\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._batch_encode_plus\u001b[1;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[0;32m    420\u001b[0m \u001b[38;5;66;03m# Set the truncation and padding strategy and restore the initial configuration\u001b[39;00m\n\u001b[0;32m    421\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_truncation_and_padding(\n\u001b[0;32m    422\u001b[0m     padding_strategy\u001b[38;5;241m=\u001b[39mpadding_strategy,\n\u001b[0;32m    423\u001b[0m     truncation_strategy\u001b[38;5;241m=\u001b[39mtruncation_strategy,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    426\u001b[0m     pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m    427\u001b[0m )\n\u001b[1;32m--> 429\u001b[0m encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    430\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    431\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    432\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_pretokenized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    433\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    435\u001b[0m \u001b[38;5;66;03m# Convert encoding to dict\u001b[39;00m\n\u001b[0;32m    436\u001b[0m \u001b[38;5;66;03m# `Tokens` has type: Tuple[\u001b[39;00m\n\u001b[0;32m    437\u001b[0m \u001b[38;5;66;03m#                       List[Dict[str, List[List[int]]]] or List[Dict[str, 2D-Tensor]],\u001b[39;00m\n\u001b[0;32m    438\u001b[0m \u001b[38;5;66;03m#                       List[EncodingFast]\u001b[39;00m\n\u001b[0;32m    439\u001b[0m \u001b[38;5;66;03m#                    ]\u001b[39;00m\n\u001b[0;32m    440\u001b[0m \u001b[38;5;66;03m# with nested dimensions corresponding to batch, overflows, sequence length\u001b[39;00m\n\u001b[0;32m    441\u001b[0m tokens_and_encodings \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    442\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_encoding(\n\u001b[0;32m    443\u001b[0m         encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    452\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m encoding \u001b[38;5;129;01min\u001b[39;00m encodings\n\u001b[0;32m    453\u001b[0m ]\n",
      "\u001b[1;31mTypeError\u001b[0m: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]"
     ]
    }
   ],
   "source": [
    "#original text\n",
    "text = df_train[\"sentence\"].values.tolist()\n",
    "\n",
    "#toeknized text\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "text_tokenized = tokenizer(text , padding = \"max_length\" , truncation = True, return_tensors = \"pt\" )\n",
    "\n",
    "#mapping token to original word\n",
    "word_ids = text_tokenized.word_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79c747c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DistilbertNER(len(unique_tags))\n",
    "learn = False\n",
    "\n",
    "try :\n",
    "    model = torch.load(\"NER_model\", map_location=torch.device('cpu'))\n",
    "except FileNotFoundError as e : \n",
    "    print(\"MODEL is not exist so new MODEL will be created\")\n",
    "    learn = True\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdd320b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = DistilbertNER(len(unique_tags))\n",
    "#Prevent Catastrofic Forgetting\n",
    "#model = freeze_model(model, num_layers = 2)\n",
    "\n",
    "#datasets\n",
    "train_dataset = NerDataset(df_train)\n",
    "dev_dataset = NerDataset(df_dev)\n",
    "\n",
    "lr = 1e-2\n",
    "optimizer = SGD(model.parameters(), lr=lr, momentum = 0.9)  \n",
    "\n",
    "#MAIN\n",
    "parameters = {\n",
    "    \"model\": model,\n",
    "    \"train_dataset\": train_dataset,\n",
    "    \"dev_dataset\" : dev_dataset,\n",
    "    \"optimizer\" : optimizer,\n",
    "    \"batch_size\" : 16,\n",
    "    \"epochs\" : 8\n",
    "}\n",
    "\n",
    "if learn == True:\n",
    "    train_loop(**parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52be4953",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.save(model, \"NER_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8035d6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_word_ids(texts):\n",
    "  \n",
    "    tokenized_inputs = tokenizer(texts, padding='max_length', max_length=512, truncation=True)\n",
    "\n",
    "    word_ids = tokenized_inputs.word_ids()\n",
    "\n",
    "    previous_word_idx = None\n",
    "    label_ids = []\n",
    "\n",
    "    for word_idx in word_ids:\n",
    "\n",
    "        if word_idx is None:\n",
    "            label_ids.append(-100)\n",
    "\n",
    "        elif word_idx != previous_word_idx:\n",
    "            try:\n",
    "                label_ids.append(1)\n",
    "            except:\n",
    "                label_ids.append(-100)\n",
    "        else:\n",
    "            try:\n",
    "                label_ids.append(1 if label_all_tokens else -100)\n",
    "            except:\n",
    "                label_ids.append(-100)\n",
    "        previous_word_idx = word_idx\n",
    "\n",
    "    return label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1de9f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_one_text(model, sentence):\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "\n",
    "    text = tokenizer(sentence, padding='max_length', max_length = 512, truncation=True, return_tensors=\"pt\")\n",
    "    #print(len(text))\n",
    "\n",
    "    mask = text['attention_mask'].to(device)\n",
    "    input_id = text['input_ids'].to(device)\n",
    "    label_ids = torch.Tensor(align_word_ids(sentence)).unsqueeze(0).to(device)\n",
    "\n",
    "    logits = model(input_id, mask, None)\n",
    "    logits_clean = logits[0][label_ids != -100]\n",
    "    \n",
    "    predictions = logits_clean.argmax(dim=1).tolist()\n",
    "    prediction_label = [ids_to_labels2[i] for i in predictions]\n",
    "    return prediction_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25354b4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "input_text = input()\n",
    "\n",
    "text = input_text\n",
    "special_characters = '[-=+,#/\\?:^$.@*\\\"※~&%ㆍ!』\\\\‘|\\(\\)\\[\\]\\<\\>`\\'…》]'\n",
    "for character in special_characters:\n",
    "    text = text.replace(character, \" \" + character + \" \") \n",
    "text = re.sub(r\"\\s+\", \" \", text)\n",
    "#pre-processing\n",
    "\n",
    "label = evaluate_one_text(model, text)\n",
    "for i in range(len(label)):\n",
    "    if (i == 0) and (label[i] == 'I-PR'):\n",
    "        label[i] = 'B-PR'\n",
    "    elif (i == 0) and (label[i] == 'I-TE'):\n",
    "        label[i] = 'B-TE'\n",
    "    elif (i == 0) and (label[i] == 'I-TR'):\n",
    "        label[i] = 'B-TR'\n",
    "    elif (i != 0) and (label[i] == 'I-PR') and ((label[i - 1] != 'B-PR' and label[i - 1] != 'I-PR')):\n",
    "        label[i] = 'B-PR'\n",
    "    elif (i != 0) and (label[i] == 'I-TE') and ((label[i - 1] != 'B-TE' and label[i - 1] != 'I-TE')):\n",
    "        label[i] = 'B-TE'\n",
    "    elif (i != 0) and (label[i] == 'I-TR') and ((label[i - 1] != 'B-TR' and label[i - 1] != 'I-TR')):\n",
    "        label[i] = 'B-TR'\n",
    "#post-processing\n",
    "        \n",
    "print(text)\n",
    "print(label)\n",
    "pattern = \"[ ]\"\n",
    "words = re.split(pattern, text)\n",
    "for i in range(len(words)):\n",
    "    words[i] = str(i) + \":\"\n",
    "maps = list(map(str.__add__, words, label))\n",
    "output_text = \" \"\n",
    "for entry in maps:\n",
    "    output_text += \" \" + entry\n",
    "print(output_text)\n",
    "\n",
    "#TE = [(5,7), (21,29), (44,44), (106,106), (108,109), (125,127)]\n",
    "#PR = [(16,16), (31,32), (33,33), (75,75), (76, 76), (77,77)]\n",
    "#TR = []\n",
    "\n",
    "#James-Louise cannot execute Magnetic Resonance Imaging(MRI) because he is wearing artificial heartbeat controller(pacemaker) due to his heart disease(In detail, it is Myocardial infarction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b503f9a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
