{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d458a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import DistilBertForTokenClassification\n",
    "from torch.optim import AdamW\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import SGD\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score,f1_score, precision_score, recall_score\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d32bf110",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_word_ids(texts):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "    tokenized_inputs = tokenizer(texts, padding='max_length', max_length=512, truncation=True)\n",
    "\n",
    "    word_ids = tokenized_inputs.word_ids()\n",
    "\n",
    "    previous_word_idx = None\n",
    "    label_ids = []\n",
    "\n",
    "    for word_idx in word_ids:\n",
    "\n",
    "        if word_idx is None:\n",
    "            label_ids.append(-100)\n",
    "\n",
    "        elif word_idx != previous_word_idx:\n",
    "            try:\n",
    "                label_ids.append(1)\n",
    "            except:\n",
    "                label_ids.append(-100)\n",
    "        else:\n",
    "            try:\n",
    "                label_ids.append(1 if label_all_tokens else -100)\n",
    "            except:\n",
    "                label_ids.append(-100)\n",
    "        previous_word_idx = word_idx\n",
    "\n",
    "    return label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d69e7e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_one_text(model, sentence):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "    unique_labels2 =set()\n",
    "    unique_labels2.add('I-TR')\n",
    "    unique_labels2.add('I-PR')\n",
    "    unique_labels2.add('B-TR')\n",
    "    unique_labels2.add('I-TE')\n",
    "    unique_labels2.add('B-PR')\n",
    "    unique_labels2.add('O')\n",
    "    unique_labels2.add('B-TE')\n",
    "    print(unique_labels2)\n",
    "    ids_to_labels2 = {v: k for v, k in enumerate(sorted(unique_labels2))}\n",
    "    print(ids_to_labels2)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "\n",
    "    text = tokenizer(sentence, padding='max_length', max_length = 512, truncation=True, return_tensors=\"pt\")\n",
    "    #print(len(text))\n",
    "\n",
    "    mask = text['attention_mask'].to(device)\n",
    "    input_id = text['input_ids'].to(device)\n",
    "    label_ids = torch.Tensor(align_word_ids(sentence)).unsqueeze(0).to(device)\n",
    "\n",
    "    logits = model(input_id, mask, None)\n",
    "    logits_clean = logits[0][label_ids != -100]\n",
    "    \n",
    "    predictions = logits_clean.argmax(dim=1).tolist()\n",
    "    prediction_label = [ids_to_labels2[i] for i in predictions]\n",
    "    return prediction_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "135a22bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistilbertNER(nn.Module):\n",
    "  def __init__(self, tokens_dim):\n",
    "    super(DistilbertNER,self).__init__()\n",
    "    if type(tokens_dim) != int:\n",
    "            raise TypeError('Please tokens_dim should be an integer')\n",
    "    if tokens_dim <= 0:\n",
    "          raise ValueError('Classification layer dimension should be at least 1')\n",
    "    self.pretrained = DistilBertForTokenClassification.from_pretrained(\"distilbert-base-uncased\", num_labels = tokens_dim) #set the output of each token classifier = unique_lables\n",
    "\n",
    "  def forward(self, input_ids, attention_mask, labels = None): #labels are needed in order to compute the loss\n",
    "\n",
    "    if labels == None:\n",
    "      out = self.pretrained(input_ids = input_ids, attention_mask = attention_mask )\n",
    "      return out\n",
    "\n",
    "    out = self.pretrained(input_ids = input_ids, attention_mask = attention_mask , labels = labels)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4da06ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLPmodule:\n",
    "    input_val=[]\n",
    "    #def __init__(self,input_val):\n",
    "    #    self.input_val = []\n",
    "\n",
    "    def append_element(idx, tag1, PR2, TE2, TR2):\n",
    "    \n",
    "        if(tag1 == 'B-PR' or tag1 == 'I-PR'):\n",
    "            PR2.append(idx)\n",
    "        elif(tag1 == 'B-TE' or tag1 == 'I-TE'):\n",
    "            TE2.append(idx)\n",
    "        elif(tag1 == 'B-TR' or tag1 == 'I-TR'):\n",
    "            TR2.append(idx)\n",
    "\n",
    "    def GetNerToken(self, target):\n",
    "        model = torch.load(\"./NER_model\" , \"cpu\")\n",
    "\n",
    "        #input_text = input()\n",
    "\n",
    "        #text = input_text\n",
    "        #special_characters = '[-=+,#/\\?:^$.@*\\\"※~&%ㆍ!』\\\\‘|\\(\\)\\[\\]\\<\\>`\\'…》]'\n",
    "        #for character in special_characters:\n",
    "        #    text = text.replace(character, \" \" + character + \" \") \n",
    "        text = ' '.join(target)\n",
    "        text = re.sub(r\"\\s+\", \" \", text)\n",
    "        #pre-processing\n",
    "        if (text[0] == \" \"):\n",
    "            text = text[1:]\n",
    "        label = evaluate_one_text(model, text)\n",
    "        for i in range(len(label)):\n",
    "            if (i == 0) and (label[i] == 'I-PR'):\n",
    "                label[i] = 'B-PR'\n",
    "            elif (i == 0) and (label[i] == 'I-TE'):\n",
    "                label[i] = 'B-TE'\n",
    "            elif (i == 0) and (label[i] == 'I-TR'):\n",
    "                label[i] = 'B-TR'\n",
    "            elif (i != 0) and (label[i] == 'I-PR') and ((label[i - 1] != 'B-PR' and label[i - 1] != 'I-PR')):\n",
    "                label[i] = 'B-PR'\n",
    "            elif (i != 0) and (label[i] == 'I-TE') and ((label[i - 1] != 'B-TE' and label[i - 1] != 'I-TE')):\n",
    "                label[i] = 'B-TE'\n",
    "            elif (i != 0) and (label[i] == 'I-TR') and ((label[i - 1] != 'B-TR' and label[i - 1] != 'I-TR')):\n",
    "                label[i] = 'B-TR'\n",
    "#post-processing\n",
    "        \n",
    "        print(text)\n",
    "        print(label)\n",
    "        pattern = \"[ ]\"\n",
    "        words = re.split(pattern, text)\n",
    "        for i in range(len(words)):\n",
    "            words[i] += \"=\"\n",
    "        maps = list(map(str.__add__, words, label))\n",
    "        print(maps)\n",
    "\n",
    "#change output format\n",
    "        tagging = []\n",
    "        PR = []\n",
    "        TE = []\n",
    "        TR = []\n",
    "        PR2 = []\n",
    "        TE2 = []\n",
    "        TR2 = []\n",
    "\n",
    "        for idx in range(len(maps)):\n",
    "            if(idx < len(maps)-1):\n",
    "                word1, tag1 = maps[idx].split(\"=\")\n",
    "                word2, tag2 = maps[idx+1].split(\"=\")\n",
    "            else:\n",
    "                word1, tag1 = maps[idx].split(\"=\")\n",
    "            tagging.append(tag1)\n",
    "\n",
    "            if(idx==len(maps)-1 and (tag1 == 'I-PR' or tag1 == 'I-TE' or tag1 == 'I-TR')):\n",
    "                NLPmodule.append_element(idx, tag1,PR2, TE2, TR2)\n",
    "            if(tag1 == 'B-PR' or tag1 == 'B-TE' or tag1 == 'B-TR'):\n",
    "                NLPmodule.append_element(idx, tag1,PR2, TE2, TR2)\n",
    "                if(tag2 != 'I-PR' and tag2 != 'I-TE' and tag2 != 'I-TR' and (idx < len(maps)-1)):\n",
    "                    NLPmodule.append_element(idx, tag1,PR2, TE2, TR2)\n",
    "            elif((tag1 == 'I-PR' or tag1 == 'I-TE' or tag1 == 'I-TR') and (tag2 != 'I-PR' and tag2 != 'I-TE' and tag2 != 'I-TR') and (idx < len(maps)-1)):\n",
    "                NLPmodule.ppend_element(idx, tag1,PR2, TE2, TR2)\n",
    "\n",
    "        for i in range(0,len(PR2),2):\n",
    "            PR.append(tuple(PR2[i:i+2]))\n",
    "        for i in range(0,len(TE2),2):\n",
    "            TE.append(tuple(TE2[i:i+2]))\n",
    "        for i in range(0,len(TR2),2):\n",
    "            TR.append(tuple(TR2[i:i+2]))\n",
    "\n",
    "        print(tagging)\n",
    "        print(\"PR =\",PR)\n",
    "        print(\"TE =\",TE)\n",
    "        print(\"TR =\",TR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f87d76c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'have', 'a', 'headache']\n",
      "{'B-TE', 'B-TR', 'I-TR', 'I-PR', 'B-PR', 'O', 'I-TE'}\n",
      "{0: 'B-PR', 1: 'B-TE', 2: 'B-TR', 3: 'I-PR', 4: 'I-TE', 5: 'I-TR', 6: 'O'}\n",
      "I have a headache\n",
      "['O', 'O', 'B-PR', 'I-PR']\n",
      "['I=O', 'have=O', 'a=B-PR', 'headache=I-PR']\n",
      "['O', 'O', 'B-PR', 'I-PR']\n",
      "PR = [(2, 3)]\n",
      "TE = []\n",
      "TR = []\n"
     ]
    }
   ],
   "source": [
    "instance = NLPmodule()\n",
    "input_list = ['I', 'have', 'a', 'headache']\n",
    "instance.input_val = input_list\n",
    "print(input_list)\n",
    "\n",
    "instance.GetNerToken(input_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
